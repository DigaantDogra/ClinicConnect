{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "     # Set up training parameters and manage the training loop.\n",
    "    TrainingArguments, Trainer,\n",
    " # Configure quantization (4‑bit precision) to reduce memory usage.\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "# Set up and apply LoRA to the base model for efficient fine‑tuning.\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import os\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device & Quantization Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# 1. Configuration\n",
    "BASE_DIR = \"/Users/arjunanand/Documents/SE_Project/ClinicConnect/AI-Model\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"trained_models/mistral-clinicconnect\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 2. Device Configuration for macOS\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 3. Model Setup with MPS Optimization\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Tokenizer Loading\n",
    "We are using Mistral 7B v0.2 model since it has a 32k context window (vs 8k context in v0.1). In general mistral 7B is highly efficient, open-source language model that achieves competitive performance on NLP benchmarks even with small number of parameters.\n",
    "\n",
    "Have to use mps optmization since model is running on mac and does not have a dedicated GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\u001b[32m      2\u001b[39m tokenizer.pad_token = tokenizer.eos_token\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m model = prepare_model_for_kbit_training(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/SE_Project/ClinicConnect/AI-Model/ai-env/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:573\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m    572\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n\u001b[32m--> \u001b[39m\u001b[32m573\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    577\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    578\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    579\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/SE_Project/ClinicConnect/AI-Model/ai-env/lib/python3.13/site-packages/transformers/modeling_utils.py:272\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    270\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    274\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/SE_Project/ClinicConnect/AI-Model/ai-env/lib/python3.13/site-packages/transformers/modeling_utils.py:4292\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4289\u001b[39m     hf_quantizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4291\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4292\u001b[39m     \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4293\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4294\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4296\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4297\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4298\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4299\u001b[39m     torch_dtype = hf_quantizer.update_torch_dtype(torch_dtype)\n\u001b[32m   4300\u001b[39m     device_map = hf_quantizer.update_device_map(device_map)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/SE_Project/ClinicConnect/AI-Model/ai-env/lib/python3.13/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:76\u001b[39m, in \u001b[36mBnb4BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     73\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing `bitsandbytes` 4-bit quantization requires Accelerate: `pip install \u001b[39m\u001b[33m'\u001b[39m\u001b[33maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m     )\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bitsandbytes_available():\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     77\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUsing `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     78\u001b[39m     )\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_bnb_backend_availability\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n",
      "\u001b[31mImportError\u001b[39m: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": device},\n",
    "    use_safetensors=True\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA (Low-Rank Adaptation) Configuration and Application\n",
    "LoRA method has been used to fine tune our mistral 7B model that only changes a small number of trainable parameters, significantly reducing memory and computational requirements without compromising performance.\n",
    "It decomposes a large matrix into two smaller low-rank matrices in the attention layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function formats the data into a text prompt and then tokenizes the model input format with a max length of 2048 tokens.\n",
    "def format_instruction(example):\n",
    "    # Clinical Care Plans\n",
    "    if \"care_plan\" in example:\n",
    "        care_plan = pd.json_normalize(example['care_plan']).to_markdown()\n",
    "        return {\n",
    "            \"text\": f\"\"\"<s>[INST] <<care_plan_generation>>\n",
    "            Generate comprehensive care plan for {example['primary_condition']['name']} ({example['primary_condition']['subtype']}):\n",
    "            Patient Profile: {example['demographics']}\n",
    "            Comorbidities: {', '.join(example['comorbidities']) if example['comorbidities'] else 'None'}\n",
    "            [/INST]\n",
    "            {care_plan}</s>\"\"\"\n",
    "        }\n",
    "    \n",
    "    # Doctor Q&A\n",
    "    if \"input\" in example:\n",
    "        return {\n",
    "            \"text\": f\"\"\"<s>[INST] <<medical_qa>>\n",
    "            {example['instruction']}\n",
    "            Patient Description: {example['input']}\n",
    "            [/INST]\n",
    "            {example['output']}</s>\"\"\"\n",
    "        }\n",
    "    \n",
    "    # OASIS Documentation\n",
    "    return {\n",
    "        \"text\": f\"\"\"<s>[INST] <<clinical_analysis>>\n",
    "        Analyze OASIS documentation and extract key care considerations:\n",
    "        {example['text'][:2000]}... [truncated]\n",
    "        [/INST]\n",
    "        Key Findings:\n",
    "        - Requires fall risk assessment\n",
    "        - Monitor for medication interactions\n",
    "        - Schedule weekly BP checks</s>\"\"\"\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minor Data Formatting required for the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_oasis(text):\n",
    "    # Remove legal/disclaimer content\n",
    "    patterns = [\n",
    "        r\"PRA Disclosure Statement.*?Baltimore, Maryland 21244-1850\\.\",\n",
    "        r\"Adapted from:.*?NACHC\\.\\n\",\n",
    "        r\"\f\"  # Remove form feed characters\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, \"\", text, flags=re.DOTALL)\n",
    "    \n",
    "    # Extract structured components\n",
    "    sections = re.split(r\"(Section [A-Z]+:|Enter Code|↓)\", text)\n",
    "    cleaned = \"\\n\".join([s for s in sections if len(s.strip()) > 10])\n",
    "    \n",
    "    return cleaned[:3000]  # Keep most relevant parts\n",
    "\n",
    "def normalize_qa(example):\n",
    "    # Standardize medical terms\n",
    "    medical_mapping = {\n",
    "        \"panadol\": \"acetaminophen\",\n",
    "        \"Z&D\": \"zinc supplementation\"\n",
    "    }\n",
    "    \n",
    "    for term, replacement in medical_mapping.items():\n",
    "        example['input'] = re.sub(rf\"\\b{term}\\b\", replacement, example['input'], flags=re.I)\n",
    "        example['output'] = re.sub(rf\"\\b{term}\\b\", replacement, example['output'], flags=re.I)\n",
    "    \n",
    "    # Add clinical context\n",
    "    example['instruction'] = \"As a board-certified physician, provide evidence-based recommendations for:\"\n",
    "    \n",
    "    return example\n",
    "\n",
    "def structure_careplan(example):\n",
    "    plan = example['care_plan']\n",
    "    return {\n",
    "        \"monitoring_schedule\": \"\\n- \".join(plan['monitoring']),\n",
    "        \"medications\": {\n",
    "            \"oral\": \"\\n- \".join(plan['medications']['oral']),\n",
    "            \"injectable\": \"\\n- \".join(plan['medications']['injectable'])\n",
    "        },\n",
    "        \"lifestyle_recommendations\": \"\\n- \".join(plan['lifestyle'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Concatenate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process datasets\n",
    "oasis = load_dataset(\"text\", data_files=\"clean_oasis.txt\") \\\n",
    "    .map(lambda x: {\"text\": clean_oasis(x[\"text\"])})\n",
    "\n",
    "doctor_qa = load_dataset(\"csv\", data_files=\"Doctor-Healthcare-100k.csv\") \\\n",
    "    .map(normalize_qa)\n",
    "\n",
    "careplans = load_dataset(\"csv\", data_files=\"clinical_care_plans.csv\") \\\n",
    "    .map(structure_careplan)\n",
    "\n",
    "# Combine with weighting\n",
    "combined = concatenate_datasets([\n",
    "    oasis[\"train\"].shuffle(seed=42).select(range(5000)),\n",
    "    doctor_qa[\"train\"].shuffle(seed=42),\n",
    "    careplans[\"train\"].shuffle(seed=42).select(range(10000))\n",
    "])\n",
    "\n",
    "# Final formatting\n",
    "tokenized_data = combined.map(format_instruction).map(\n",
    "    lambda x: tokenizer(\n",
    "        x[\"text\"],\n",
    "        max_length=2048,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ),\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Setup & Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data,\n",
    "    data_collator=lambda data: {\n",
    "        \"input_ids\": torch.stack([d[\"input_ids\"] for d in data]),\n",
    "        \"attention_mask\": torch.stack([d[\"attention_mask\"] for d in data]),\n",
    "        \"labels\": torch.stack([d[\"input_ids\"] for d in data])\n",
    "    }\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save Model\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"Model saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_care_plan(patient_profile):\n",
    "    prompt = f\"\"\"<s>[INST] <<care_plan_generation>>\n",
    "    Generate comprehensive care plan for:\n",
    "    Patient Profile: {patient_profile}\n",
    "    [/INST]\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test with sample input\n",
    "test_profile = \"65yo Male, T2DM, HbA1c 8.5%, CKD Stage 3, Hypertension\"\n",
    "print(\"Sample Care Plan:\")\n",
    "print(generate_care_plan(test_profile))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
