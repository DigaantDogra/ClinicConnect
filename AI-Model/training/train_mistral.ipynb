{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhmfJVcg54Ur"
      },
      "source": [
        "# Importing Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gz5Y8h5e54Ur",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets\n",
        "!pip install -q transformers peft accelerate bitsandbytes datasets\n",
        "\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "     # Set up training parameters and manage the training loop.\n",
        "    TrainingArguments, Trainer,\n",
        " # Configure quantization (4‑bit precision) to reduce memory usage.\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "# Set up and apply LoRA to the base model for efficient fine‑tuning.\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "import os\n",
        "import re\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4iBF_JTp67zO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "qQTRqx2W9Fwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTjWZAXF54Us"
      },
      "source": [
        "# Device & Quantization Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljB36PDH54Us"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "# 1. Configuration: Set directories in Google Drive\n",
        "BASE_DIR = \"/content/drive/MyDrive/ClinicConnect\"\n",
        "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
        "OUTPUT_DIR = os.path.join(BASE_DIR, \"trained_models/mistral-clinicconnect\")\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# 2. Device Configuration for Colab (using CUDA)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 3. Model Setup with CUDA and 4-bit Quantization\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpNQX4Ma54Us"
      },
      "source": [
        "# Model and Tokenizer Loading\n",
        "We are using Mistral 7B v0.2 model since it has a 32k context window (vs 8k context in v0.1). In general mistral 7B is highly efficient, open-source language model that achieves competitive performance on NLP benchmarks even with small number of parameters.\n",
        "\n",
        "Have to use mps optmization since model is running on mac and does not have a dedicated GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAeJWpPo54Us"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",  # Automatically maps to available devices\n",
        "    use_safetensors=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soh3fMEq54Us"
      },
      "source": [
        "# LoRA (Low-Rank Adaptation) Configuration and Application\n",
        "LoRA method has been used to fine tune our mistral 7B model that only changes a small number of trainable parameters, significantly reducing memory and computational requirements without compromising performance.\n",
        "It decomposes a large matrix into two smaller low-rank matrices in the attention layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-Hbnaps54Us"
      },
      "outputs": [],
      "source": [
        "# 4. Configure LORA for PEFT\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1yEhVvn54Ut"
      },
      "source": [
        "# Dataset Preparation and Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYBQQG_C54Ut"
      },
      "outputs": [],
      "source": [
        "def format_care_plan(plan):\n",
        "    \"\"\"Safely format care plan data with null checks.\"\"\"\n",
        "    # Handle None input\n",
        "    if not plan:\n",
        "        return \"**No care plan available**\"\n",
        "\n",
        "    # Safely get nested values\n",
        "    monitoring = \"\\n- \".join(plan.get('monitoring', [])) or \"None\"\n",
        "    medications = plan.get('medications', {})\n",
        "    oral_meds = \"\\n- \".join(medications.get('oral', [])) or \"None\"\n",
        "    injectable_meds = \"\\n- \".join(medications.get('injectable', [])) or \"None\"\n",
        "    lifestyle = \"\\n- \".join(plan.get('lifestyle', [])) or \"None\"\n",
        "\n",
        "    return f\"\"\"\n",
        "**Monitoring:**\n",
        "- {monitoring}\n",
        "\n",
        "**Medications:**\n",
        "- Oral: {oral_meds}\n",
        "- Injectable: {injectable_meds}\n",
        "\n",
        "**Lifestyle Recommendations:**\n",
        "- {lifestyle}\n",
        "\"\"\"\n",
        "\n",
        "def format_instruction(example):\n",
        "    \"\"\"Format data into text prompts with validation.\"\"\"\n",
        "    try:\n",
        "        if \"care_plan\" in example:\n",
        "            # Validate care plan structure\n",
        "            care_plan_data = example.get('care_plan') or {}\n",
        "            primary_condition = care_plan_data.get('primary_condition', {})\n",
        "\n",
        "            return {\n",
        "                \"text\": f\"\"\"<s>[INST] Generate care plan for {primary_condition.get('name', 'unknown')}:\n",
        "                Patient: {care_plan_data.get('demographics', 'No demographics')}\n",
        "                Comorbidities: {', '.join(care_plan_data.get('comorbidities', [])) or 'None'}\n",
        "                [/INST]\n",
        "                {format_care_plan(care_plan_data.get('care_plan', {}))}</s>\"\"\"\n",
        "            }\n",
        "\n",
        "        elif \"input\" in example and \"output\" in example:\n",
        "            return {\n",
        "                \"text\": f\"\"\"<s>[INST] {example.get('instruction', 'Medical question:')}\n",
        "                {example['input']}\n",
        "                [/INST]\n",
        "                {example.get('output', 'No response available')}</s>\"\"\"\n",
        "            }\n",
        "\n",
        "        elif \"text\" in example:\n",
        "            return {\n",
        "                \"text\": f\"\"\"<s>[INST] Analyze clinical documentation:\n",
        "                {example['text'][:2000]}\n",
        "                [/INST]\n",
        "                Key considerations: [Extracted from OASIS data]</s>\"\"\"\n",
        "            }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error formatting example: {e}\")\n",
        "        return {\"text\": \"<s>[INST] Invalid data format [/INST] Error in example</s>\"}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li2Av9Q154Ut"
      },
      "source": [
        "## Minor Data Formatting required for the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VafGzlTN54Ut"
      },
      "outputs": [],
      "source": [
        "# 5. Data Processing Functions\n",
        "def clean_oasis(text):\n",
        "    \"\"\"Clean OASIS documentation by removing disclaimers and extracting relevant sections.\"\"\"\n",
        "    patterns = [\n",
        "        r\"PRA Disclosure Statement.*?Baltimore, Maryland 21244-1850\\.\",\n",
        "        r\"Adapted from:.*?NACHC\\.\\n\",\n",
        "        r\"\\f\"  # Remove form feed characters\n",
        "    ]\n",
        "    for pattern in patterns:\n",
        "        text = re.sub(pattern, \"\", text, flags=re.DOTALL)\n",
        "\n",
        "    sections = re.split(r\"(Section [A-Z]+:|Enter Code|↓)\", text)\n",
        "    cleaned = \"\\n\".join([s for s in sections if len(s.strip()) > 10])\n",
        "\n",
        "    return cleaned[:3000]  # Limit to most relevant parts\n",
        "\n",
        "def normalize_qa(example):\n",
        "    \"\"\"Normalize Q&A data by standardizing medical terms and adding clinical context.\"\"\"\n",
        "    medical_mapping = {\n",
        "        \"panadol\": \"acetaminophen\",\n",
        "        \"Z&D\": \"zinc supplementation\"\n",
        "    }\n",
        "\n",
        "    for term, replacement in medical_mapping.items():\n",
        "        example['input'] = re.sub(rf\"\\b{term}\\b\", replacement, example['input'], flags=re.I)\n",
        "        example['output'] = re.sub(rf\"\\b{term}\\b\", replacement, example['output'], flags=re.I)\n",
        "\n",
        "    example['instruction'] = \"As a board-certified physician, provide evidence-based recommendations for:\"\n",
        "    return example\n",
        "\n",
        "import ast\n",
        "import json\n",
        "\n",
        "def structure_careplan(example):\n",
        "    # Handle care_plan as above\n",
        "    care_plan = example.get('care_plan')\n",
        "    if isinstance(care_plan, str):\n",
        "        if care_plan.strip() == \"None\":\n",
        "            plan = {}\n",
        "        else:\n",
        "            try:\n",
        "                plan = ast.literal_eval(care_plan)\n",
        "            except (ValueError, SyntaxError):\n",
        "                plan = {}\n",
        "    elif isinstance(care_plan, dict):\n",
        "        plan = care_plan\n",
        "    else:\n",
        "        plan = {}\n",
        "\n",
        "    # Clean comorbidities\n",
        "    comorbidities = example.get('comorbidities', [])\n",
        "    if isinstance(comorbidities, str):\n",
        "        try:\n",
        "            comorbidities = ast.literal_eval(comorbidities)\n",
        "        except (ValueError, SyntaxError):\n",
        "            comorbidities = []\n",
        "    # Remove 'none' from the list, treating it as no comorbidities\n",
        "    if isinstance(comorbidities, list) and 'none' in comorbidities:\n",
        "        comorbidities = [c for c in comorbidities if c != 'none']\n",
        "        if not comorbidities:  # If only 'none' was present, make it empty\n",
        "            comorbidities = []\n",
        "\n",
        "    return {\n",
        "        \"care_plan\": {\n",
        "            \"monitoring\": plan.get('monitoring', []),\n",
        "            \"medications\": plan.get('medications', {}),\n",
        "            \"lifestyle\": plan.get('lifestyle', [])\n",
        "        },\n",
        "        \"comorbidities\": comorbidities\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX2HUEI-54Ut"
      },
      "source": [
        "## Load and Concatenate Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qNP0GuS54Ut"
      },
      "outputs": [],
      "source": [
        "# # Load OASIS text data, cleaning it first\n",
        "# oasis = load_dataset(\"text\", data_files=os.path.join(DATA_DIR, \"clean_oasis.txt\")).map(lambda x: {\"text\": clean_oasis(x[\"text\"])})\n",
        "\n",
        "# # Load Doctor Q&A data and normalize it\n",
        "# doctor_qa = load_dataset(\"csv\", data_files=os.path.join(DATA_DIR, \"Doctor-HealthCare-100k.csv\")).map(normalize_qa)\n",
        "\n",
        "# # Load Clinical Care Plans data and restructure it\n",
        "# careplans = load_dataset(\"csv\", data_files=os.path.join(DATA_DIR, \"clinical_care_plans.csv\")).map(structure_careplan)\n",
        "\n",
        "# # Combine datasets (with shuffling and selection for weighting)\n",
        "# oasis_size = min(5000, len(oasis[\"train\"]))\n",
        "# careplans_size = min(10000, len(careplans[\"train\"]))\n",
        "\n",
        "# combined = concatenate_datasets([\n",
        "#     oasis[\"train\"].shuffle(seed=42).select(range(oasis_size)),\n",
        "#     doctor_qa[\"train\"].shuffle(seed=42),\n",
        "#     careplans[\"train\"].shuffle(seed=42).select(range(careplans_size))\n",
        "# ])\n",
        "oasis = load_dataset(\"text\", data_files=os.path.join(DATA_DIR, \"clean_oasis.txt\")).map(lambda x: {\"text\": clean_oasis(x[\"text\"])})\n",
        "doctor_qa = load_dataset(\"csv\", data_files=os.path.join(DATA_DIR, \"Doctor-HealthCare-100k.csv\")).map(normalize_qa)\n",
        "careplans = load_dataset(\"csv\", data_files=os.path.join(DATA_DIR, \"clinical_care_plans.csv\")).map(structure_careplan)\n",
        "\n",
        "combined = concatenate_datasets([\n",
        "    oasis[\"train\"].shuffle(seed=42).select(range(min(5000, len(oasis[\"train\"])))),\n",
        "    doctor_qa[\"train\"].shuffle(seed=42),\n",
        "    careplans[\"train\"].shuffle(seed=42).select(range(min(10000, len(careplans[\"train\"]))))\n",
        "])\n",
        "\n",
        "# Format, remove unused columns, and filter out short examples\n",
        "formatted = combined.map(\n",
        "    lambda x: format_instruction(x) or {\"text\": \"\"},\n",
        "    remove_columns=combined.column_names\n",
        ").filter(lambda x: len(x[\"text\"]) > 50)\n",
        "\n",
        "# Tokenize the formatted text prompts\n",
        "tokenized_data = formatted.map(\n",
        "    lambda x: tokenizer(\n",
        "        x[\"text\"],\n",
        "        max_length=2048,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ),\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting Data into Training and Testing Sets"
      ],
      "metadata": {
        "id": "Y1gkpP7ZC61z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split tokenized_data into training and evaluation datasets.\n",
        "split_data = tokenized_data.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = split_data[\"train\"]\n",
        "eval_dataset = split_data[\"test\"]"
      ],
      "metadata": {
        "id": "ZS3J-LpBHlsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGp3SDMR54Ut"
      },
      "source": [
        "# Training Setup & Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Phd7kqmb54Ut"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "model.gradient_checkpointing_enable()\n",
        "model.enable_input_require_grads()\n",
        "model.config.use_cache = False\n",
        "\n",
        "# 6. Training Setup with Evaluation Strategy\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    learning_rate=2e-5,\n",
        "    fp16=True,\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"steps\",  # Updated to avoid FutureWarning\n",
        "    eval_steps=500,\n",
        "    report_to=\"none\",\n",
        "    push_to_hub=False\n",
        ")\n",
        "\n",
        "# 7. Initialize Trainer with corrected data_collator\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=lambda data: {\n",
        "        \"input_ids\": torch.tensor([d[\"input_ids\"] for d in data]),\n",
        "        \"attention_mask\": torch.tensor([d[\"attention_mask\"] for d in data]),\n",
        "        \"labels\": torch.tensor([d[\"input_ids\"] for d in data])  # Causal LM: labels = input_ids\n",
        "    }\n",
        ")\n",
        "\n",
        "# 8. Start Training\n",
        "print(\"Starting training...\")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyuzcBQv54Ut"
      },
      "source": [
        "# Test Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQNr0uj854Ut"
      },
      "outputs": [],
      "source": [
        "# 12. Inference Function\n",
        "def generate_care_plan(patient_profile):\n",
        "    \"\"\"Generate a care plan based on patient profile.\"\"\"\n",
        "    prompt = f\"\"\"<s>[INST] <<care_plan_generation>>\n",
        "    Generate comprehensive care plan for:\n",
        "    Patient Profile: {patient_profile}\n",
        "    [/INST]\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Test with sample input\n",
        "test_profile = \"65yo Male, T2DM, HbA1c 8.5%, CKD Stage 3, Hypertension\"\n",
        "print(\"Sample Care Plan:\")\n",
        "print(generate_care_plan(test_profile))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}