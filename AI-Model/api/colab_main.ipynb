{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q fastapi nest-asyncio pyngrok uvicorn python-multipart transformers accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "g22tlomu9D4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BioMistral Model\n",
        "\n",
        "Since this model is already trained on healthcare datasets it's quite excellent for our project and the testing results have proven so."
      ],
      "metadata": {
        "id": "758IeMq29iLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile careplan_api.py\n",
        "import torch\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "import os\n",
        "\n",
        "# Configuration\n",
        "MODEL_NAME = \"BioMistral/BioMistral-7B-DARE\"\n",
        "PORT = 8000\n",
        "NGROK_TOKEN = \"2vVORBaJkFe0IQePzAisOOvcmYo_7XZQVXi4VWTa2BoScr5GH\"  # Get from https://dashboard.ngrok.com\n",
        "\n",
        "# Cleanup\n",
        "os.system(f\"fuser -k {PORT}/tcp > /dev/null 2>&1\")\n",
        "ngrok.kill()\n",
        "\n",
        "# Model Loading\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_4bit=True  # Reduces memory usage\n",
        ")\n",
        "\n",
        "# FastAPI Setup\n",
        "app = FastAPI(title=\"BioMistral Care Plan Generator\")\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "class PatientRequest(BaseModel):\n",
        "    profile: str\n",
        "    condition: str\n",
        "    subtype: str\n",
        "    comorbidities: list[str]\n",
        "\n",
        "def format_prompt(request: PatientRequest):\n",
        "    return f\"\"\"<s>[INST] Generate comprehensive care plan for {request.condition} ({request.subtype}):\n",
        "Patient: {request.profile}\n",
        "Comorbidities: {', '.join(request.comorbidities) if request.comorbidities else 'None'}\n",
        "Format response with EXACT sections:\n",
        "### Monitoring\n",
        "### Medications\n",
        "### Lifestyle [/INST]\"\"\"\n",
        "\n",
        "@app.post(\"/generate\")\n",
        "async def generate_plan(request: PatientRequest):\n",
        "    try:\n",
        "        prompt = format_prompt(request)\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        care_plan = full_response.split(\"[/INST]\", 1)[-1].strip()\n",
        "\n",
        "        return {\"plan\": care_plan}\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    return {\"status\": \"healthy\", \"model\": MODEL_NAME}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Start ngrok tunnel\n",
        "    ngrok.set_auth_token(NGROK_TOKEN)\n",
        "    public_url = ngrok.connect(PORT, \"http\").public_url\n",
        "    print(f\"\\nAPI Accessible at: {public_url}\\n\")\n",
        "\n",
        "    # Start server\n",
        "    nest_asyncio.apply()\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=PORT)"
      ],
      "metadata": {
        "id": "q2LM55i59IAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code is from our personal trained Mistal model on healthcare datasets. Over the API because there is a problem with how the merging is being done for the msitral model it's giving gibberish output.\n",
        "Testing it locally is giving excellent output. Due to time constraints we are loading an already trained BioMistral model from hugging face."
      ],
      "metadata": {
        "id": "H2ZTQkIe9KUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Install core dependencies\n",
        "# !pip install -q transformers accelerate bitsandbytes peft\n",
        "\n",
        "# # Mount Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# import torch\n",
        "# from peft import PeftModel, PeftConfig\n",
        "# from transformers import (\n",
        "#     AutoTokenizer,\n",
        "#     AutoModelForCausalLM,\n",
        "#     BitsAndBytesConfig\n",
        "# )\n",
        "\n",
        "# # Configuration\n",
        "# PEFT_MODEL_PATH = \"/content/drive/MyDrive/ClinicConnect/trained_models/mistral-clinicconnect\"\n",
        "# BASE_MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "# # Load model function\n",
        "# def load_model():\n",
        "#     # Load PEFT config\n",
        "#     config = PeftConfig.from_pretrained(PEFT_MODEL_PATH)\n",
        "\n",
        "#     # Quantization config (MUST match training setup)\n",
        "#     bnb_config = BitsAndBytesConfig(\n",
        "#         load_in_4bit=True,\n",
        "#         bnb_4bit_quant_type=\"nf4\",\n",
        "#         bnb_4bit_compute_dtype=torch.float16,\n",
        "#         bnb_4bit_use_double_quant=True\n",
        "#     )\n",
        "\n",
        "#     # Load base model\n",
        "#     base_model = AutoModelForCausalLM.from_pretrained(\n",
        "#         BASE_MODEL_NAME,\n",
        "#         quantization_config=bnb_config,\n",
        "#         device_map=\"auto\",\n",
        "#         trust_remote_code=True\n",
        "#     )\n",
        "\n",
        "#     # Load PEFT adapter\n",
        "#     model = PeftModel.from_pretrained(\n",
        "#         base_model,\n",
        "#         PEFT_MODEL_PATH,\n",
        "#         device_map=\"auto\"\n",
        "#     )\n",
        "#     model.eval()\n",
        "\n",
        "#     # Load tokenizer\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
        "#     tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "#     return model, tokenizer\n",
        "\n",
        "# # Load the model\n",
        "# model, tokenizer = load_model()\n",
        "\n",
        "# # Test generation function\n",
        "# def test_generation(patient_profile, condition, subtype, comorbidities):\n",
        "#     prompt = f\"\"\"<s>[INST] Generate care plan for {condition} ({subtype}):\n",
        "# Patient: {patient_profile}\n",
        "# Comorbidities: {', '.join(comorbidities) if comorbidities else 'None'}\n",
        "# Format Response With:\n",
        "# **Monitoring**, **Medications**, **Lifestyle** [/INST]\"\"\"\n",
        "\n",
        "#     inputs = tokenizer(\n",
        "#         prompt,\n",
        "#         return_tensors=\"pt\",\n",
        "#         max_length=2048,\n",
        "#         truncation=True\n",
        "#     ).to(model.device)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model.generate(\n",
        "#             **inputs,\n",
        "#             max_new_tokens=512,\n",
        "#             temperature=0.7,\n",
        "#             top_p=0.85,\n",
        "#             do_sample=True,\n",
        "#             repetition_penalty=1.15,\n",
        "#             pad_token_id=tokenizer.eos_token_id\n",
        "#         )\n",
        "\n",
        "#     full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "#     # Extract generated text\n",
        "#     if \"[/INST]\" in full_response:\n",
        "#         return full_response.split(\"[/INST]\")[-1].strip()\n",
        "#     return full_response\n",
        "\n",
        "# # Test case\n",
        "# test_data = {\n",
        "#     \"patient_profile\": \"65yo Male, T2DM, HbA1c 8.5%, CKD Stage 3, Hypertension\",\n",
        "#     \"condition\": \"diabetes\",\n",
        "#     \"subtype\": \"Type 2\",\n",
        "#     \"comorbidities\": [\"CKD Stage 3\", \"Hypertension\"]\n",
        "# }\n",
        "\n",
        "# # Run test\n",
        "# print(\"Testing model generation...\\n\")\n",
        "# result = test_generation(**test_data)\n",
        "\n",
        "# print(\"Generated Care Plan:\")\n",
        "# print(result)\n",
        "\n",
        "# # Validation check\n",
        "# required_sections = [\"Monitoring\", \"Medications\", \"Lifestyle\"]\n",
        "# missing = [section for section in required_sections if section not in result]\n",
        "# print(\"\\nValidation:\")\n",
        "# print(f\"Missing sections: {missing if missing else 'None'}\")\n",
        "\n",
        "# # Memory cleanup\n",
        "# torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "cVj8HeEd9Jsr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}